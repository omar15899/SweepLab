#!/bin/bash
# run_heat_big.pbs  (flat copy with path-flattening)
set -euo pipefail

# ---------- submit array ----------
if [ -z "${PBS_JOBID:-}" ]; then
  if [ "${SMOKE:-0}" = "1" ]; then
    TOTAL=4
  else
    TOTAL=$(python3 - <<'PY'
NCELLS=[2,4,8,16,32,64,128,256,512]
DT=[1e-1,1e-2,1e-3,1e-4,1e-5]
M=list(range(1,11))
print(len(NCELLS)*len(DT)*len(M))
PY
)
  fi

  choose_queue() {
    if [ -n "${FORCE_QUEUE:-}" ]; then
      echo "$FORCE_QUEUE"; return
    fi
    local best="medium" min=999999
    while read -r q mem cput wt node run que lm state; do
      [[ -z "$q" || "$q" == Queue || "$q" == "-----" ]] && continue
      case "$q" in
        firedrake|medium|jumbo|large32)
          local load=$(( ${run:-0} + ${que:-0} ))
          if [ "$load" -lt "$min" ]; then min=$load; best=$q; fi
          ;;
      esac
    done < <(qstat -q 2>/dev/null || true)
    echo "${best:-medium}"
  }
  Q=$(choose_queue)

  declare -A MEM=(
    [medium]=7900mb
    [firedrake]=12900mb
    [jumbo]=15500mb
    [large32]=32gb
  )
  MAX_SIM=30

  echo "Submitting array of $TOTAL to queue: $Q (mem=${MEM[$Q]:-7900mb}, max ${MAX_SIM} concurrent)"

  exec qsub -q "$Q" \
    -N run_heat_big \
    -l select=1:ncpus=1:mem="${MEM[$Q]:-7900mb}" \
    -l walltime=100:00:00 \
    -m bea -M ok24@ic.ac.uk \
    -j oe \
    -o /home/ma/o/ok24/logs \
    -v SMOKE="${SMOKE:-0}" \
    -v FORCE_QUEUE="${FORCE_QUEUE:-}" \
    -t 0-$((TOTAL-1))%${MAX_SIM} \
    "$0"
fi

# ---------- runtime context ----------
set -x
umask 0022
echo "[START] $(date) host=$(hostname) job=${PBS_JOBID} arr=${PBS_ARRAYID:-0} user=${USER}"

export OMP_NUM_THREADS=1 OPENBLAS_NUM_THREADS=1 MKL_NUM_THREADS=1 \
       BLIS_NUM_THREADS=1 VECLIB_MAXIMUM_THREADS=1 NUMEXPR_NUM_THREADS=1 \
       GOTO_NUM_THREADS=1

source /usr/local/firedrake/bin/activate

# caches en HOME
export HOME=/home/ma/o/ok24
export XDG_CACHE_HOME="$HOME/.cache"
export PYOP2_CACHE_DIR="$XDG_CACHE_HOME/pyop2"
export TSFC_KERNEL_CACHE_DIR="$XDG_CACHE_HOME/tsfc"
export TSFC_CACHE_DIR="$XDG_CACHE_HOME/tsfc"
export FIREDRAKE_TSFC_KERNEL_CACHE_DIR="$XDG_CACHE_HOME/tsfc"
export PYOP2_ALWAYS_REBUILD=1
mkdir -p "$PYOP2_CACHE_DIR" "$TSFC_KERNEL_CACHE_DIR" /home/ma/o/ok24/logs

cd /home/ma/o/ok24
which python3
python3 -V

# ---------- staging + dest ----------
STAGE="/tmp/sdc-${USER}-${PBS_JOBID}.${PBS_ARRAYID:-0}"
FLAT="$STAGE/__flat__"
DEST="/home/clustor2/ma/o/ok24/solver_results/heatfiles/all_cluster_runs"
mkdir -p "$STAGE" "$FLAT" "$DEST"

# firedrake escribe en STAGE
export SDC_OUTPUT_DIR="$STAGE"
# export dest para flush desde python (opcional)
export SDC_DEST_DIR="$DEST"

# ---------- flatten + rsync ----------
rsync_flatten_and_push() {
  local log="/home/ma/o/ok24/logs/rsync_${PBS_JOBID}.${PBS_ARRAYID:-0}.log"
  {
    echo "[RSYNC] listing $STAGE"
    mkdir -p "$FLAT"   # asegura que __flat__ existe incluso si alguien lo borro antes
    find "$STAGE" -type f \( -name '*.h5' -o -name '*.json' -o -name '*_log.txt' -o -name '*__log.txt' \) -print0 \
    | while IFS= read -r -d '' f; do
        rel="${f#"$STAGE/"}"
        out="${rel//\//__}"
        mv -f -- "$f" "$FLAT/$out" || true
      done
    echo "[RSYNC] -> $DEST"
    rsync -av --remove-source-files "$FLAT"/ "$DEST"/ || true
    echo "[RSYNC] done"
  } >>"$log" 2>&1
  # limpia directorios vacios de STAGE (pero no borra __flat__)
  find "$STAGE" -type d -not -path "$FLAT" -empty -delete || true
}

trap 'echo "[TRAP] flushing on exit"; rsync_flatten_and_push' EXIT

# ---------- python work ----------
python3 -u - <<'PY'
import os, json, runpy, shutil, subprocess
from itertools import product
from pathlib import Path

SMOKE = os.environ.get("SMOKE","0") == "1"

if SMOKE:
    NCELLS=[8]
    DT=[5e-2]
    M=[6,6]
else:
    NCELLS=[2,4,8,16,32,64,128,256,512]
    DT=[1e-1,1e-2,1e-3,1e-4,1e-5]
    M=list(range(1,11))

TFINAL=1.0
DEGREE_LIST=[1,2,3,4]  # barrido de degree 1..4

idx = int(os.environ.get("PBS_ARRAYID","0"))
combos = list(product(NCELLS, DT, M))
n, dt, m = combos[idx]
sweeps = m
print(f"[ARR] idx={idx} n={n} dt={dt} M={m} sweeps={sweeps}")

stage = Path(os.environ.get("SDC_OUTPUT_DIR","/tmp"))
flat  = stage / "__flat__"
dest  = Path(os.environ.get("SDC_DEST_DIR","/home/clustor2"))
flat.mkdir(parents=True, exist_ok=True)

def py_flatten_and_push():
    # aplanar cualquier fichero interesante bajo STAGE
    for root, dirs, files in os.walk(stage):
        for name in files:
            if not (name.endswith(".h5") or name.endswith(".json") or name.endswith("_log.txt") or name.endswith("__log.txt")):
                continue
            src = Path(root) / name
            try:
                rel = src.relative_to(stage).as_posix()
            except Exception:
                continue
            out = rel.replace("/", "__")
            try:
                shutil.move(str(src), str(flat / out))
            except Exception:
                pass
    # rsync a DEST
    subprocess.run(["rsync","-av","--remove-source-files", str(flat)+"/", str(dest)+"/"], check=False)
    # limpiar vacios bajo STAGE sin tocar __flat__
    try:
        for p in sorted(stage.rglob("*"), reverse=True):
            if p.is_dir() and p.name != "__flat__":
                try: p.rmdir()
                except Exception: pass
    except Exception:
        pass

variants = []
for prectype in ("MIN-SR-FLEX", "MIN-SR-NS"):
    for is_par in (True, False):
        for deg in DEGREE_LIST:
            params = {
                "n_cells": n,
                "dt": dt,
                "M": m,
                "sweeps": sweeps,
                "Tfinal": TFINAL,
                "degree": deg,
                "is_parallel": is_par,
                "prectype": prectype,
                "analysis": True,
                "mode": "checkpoint",
                "folder_name": "all_cluster_runs",
                "path_name": str(stage),
            }
            variants.append(params)

for v in variants:
    os.environ["SDC_PARAMS_JSON"] = json.dumps(v)
    print("[RUN]", v)
    runpy.run_path("/home/clustor2/ma/o/ok24/clusterscripts/sdc_heat_equation.py",
                   run_name="__main__")
    # flush tras cada variante
    py_flatten_and_push()

print("[PY] all variants done.")
PY

# ---------- final sync ----------
rsync_flatten_and_push

# ---------- cleanup ----------
rmdir "$FLAT" 2>/dev/null || true
rmdir "$STAGE" 2>/dev/null || true

echo "[END] $(date)"