#!/bin/bash
# run_sdc_heat_ok24.pbs - single run, ASCII only
#PBS -N sdc_heat
#PBS -j oe
#PBS -m bea
#PBS -M ok24@ic.ac.uk

set -euo pipefail

# ---------- base parameters (override with qsub -v VAR=...) ----------
USR="${USR:-ok24}"
HOME_DIR="${HOME_DIR:-/home/ma/o/${USR}}"
PY_SCRIPT="${PY_SCRIPT:-${HOME_DIR}/clusterscripts/sdc_heat_equation.py}"
BASE_DEST="${BASE_DEST:-${HOME_DIR}/solver_results/heatfiles/all_cluster_runs}"
OUTDIR="${OUTDIR:-$BASE_DEST}"
LOGDIR="${LOGDIR:-${HOME_DIR}/logs}"

# solver defaults (override with -v)
DT="${DT:-5e-2}"
NCELLS="${NCELLS:-8}"
M="${M:-6}"
SWEEPS="${SWEEPS:-$M}"      # must equal M
TFINAL="${TFINAL:-0.1}"
IS_PAR="${IS_PAR:-1}"       # 1 parallel sweeps per node; 0 global
PREC="${PREC:-MIN-SR-FLEX}"
DEGREE="${DEGREE:-1}"
ANALYSIS="${ANALYSIS:-1}"
MODE="${MODE:-checkpoint}"  # "checkpoint" or "vtk"

# ---------- HDF5 and caches ----------
export HDF5_USE_FILE_LOCKING=FALSE
export HDF5_DISABLE_FILE_LOCKING=1
export HDF5_DISABLE_VERSION_CHECK=2
export HDF5_DRIVER=sec2

export PYOP2_CACHE_DIR="${PYOP2_CACHE_DIR:-$HOME_DIR/.cache/pyop2}"
export FIREDRAKE_TSFC_KERNEL_CACHE_DIR="${FIREDRAKE_TSFC_KERNEL_CACHE_DIR:-$HOME_DIR/.cache/firedrake/tsfc}"
mkdir -p "$PYOP2_CACHE_DIR" "$FIREDRAKE_TSFC_KERNEL_CACHE_DIR"

# ---------- Firedrake activation ----------
if [ -f "/usr/local/firedrake/bin/activate" ]; then
  # shellcheck disable=SC1091
  source "/usr/local/firedrake/bin/activate"
else
  echo "[ERROR] Firedrake activate not found at /usr/local/firedrake/bin/activate" >&2
  exit 2
fi

# ---------- workdir and output dirs ----------
cd "${PBS_O_WORKDIR:-$HOME_DIR}"
mkdir -p "$OUTDIR" "$LOGDIR"

# ---------- pack config for __main__ ----------
export SDC_OUTPUT_DIR="$OUTDIR"
export SDC_PARAMS_JSON="$(
  python3 - <<'PY'
import json, os
cfg = dict(
  dt=float(os.environ["DT"]),
  n_cells=int(os.environ["NCELLS"]),
  sweeps=int(os.environ["SWEEPS"]),
  M=int(os.environ["M"]),
  Tfinal=float(os.environ["TFINAL"]),
  is_parallel=bool(int(os.environ["IS_PAR"])),
  prectype=os.environ["PREC"],
  degree=int(os.environ["DEGREE"]),
  analysis=bool(int(os.environ["ANALYSIS"])),
  mode=os.environ["MODE"],
  folder_name=".",
  path_name=os.environ["OUTDIR"],
)
print(json.dumps(cfg))
PY
)"

echo "[INFO] Host=$(hostname) Queue=${PBS_QUEUE:-?} JobID=${PBS_JOBID:-?}"
echo "[INFO] OUTDIR=$OUTDIR"
echo "[INFO] PY_SCRIPT=$PY_SCRIPT"

# ---------- run ----------
python3 "$PY_SCRIPT"

# ---------- normalize artifact names ----------
cd "$OUTDIR"
latest_h5=$(ls -t *.h5 2>/dev/null | head -n1 || true)
latest_json=$(ls -t *_convergence_results.json 2>/dev/null | head -n1 || true)
latest_log=$(ls -t *_log.txt 2>/dev/null | head -n1 || true)

[ -n "${latest_h5:-}" ]   && cp -f -- "$latest_h5"  solution.h5
[ -n "${latest_json:-}" ] && cp -f -- "$latest_json" results.json
[ -n "${latest_log:-}" ]  && cp -f -- "$latest_log" log.txt

echo "[INFO] Artifacts:"
[ -f solution.h5 ]  && echo "  $(readlink -f solution.h5)"
[ -f results.json ] && echo "  $(readlink -f results.json)"
[ -f log.txt ]      && echo "  $(readlink -f log.txt)"

# ---------- keep PBS stdout copy in LOGDIR ----------
JOBNUM="${PBS_JOBID%%.*}"
PBS_O="${PBS_O_WORKDIR:-$HOME_DIR}"
if [ -f "${PBS_O}/sdc_heat.o${JOBNUM}" ]; then
  cp -f "${PBS_O}/sdc_heat.o${JOBNUM}" "${LOGDIR}/sdc_heat.o${PBS_JOBID}"
fi
